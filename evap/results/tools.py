from collections import namedtuple, defaultdict
from functools import partial
from math import ceil
from statistics import median
import itertools

from django.conf import settings
from django.core.cache import caches
from django.db.models import Sum

from evap.evaluation.models import TextAnswer, Contribution, RatingAnswerCounter
from evap.evaluation.tools import questionnaires_and_contributions


GRADE_COLORS = {
    1: (136, 191, 74),
    2: (187, 209, 84),
    3: (239, 226, 88),
    4: (242, 158, 88),
    5: (235,  89, 90),
}


# see calculate_results
ResultSection = namedtuple('ResultSection', ('questionnaire', 'contributor', 'label', 'results', 'warning'))
CommentSection = namedtuple('CommentSection', ('questionnaire', 'contributor', 'label', 'is_responsible', 'results'))
RatingResult = namedtuple('RatingResult', ('question', 'total_count', 'average', 'counts', 'warning'))
YesNoResult = namedtuple('YesNoResult', ('question', 'total_count', 'average', 'counts', 'warning', 'approval_count'))
TextResult = namedtuple('TextResult', ('question', 'answers'))
HeadingResult = namedtuple('HeadingResult', ('question'))


def avg(iterable):
    items = [item for item in iterable if item is not None]
    if not items:
        return None
    return sum(items) / len(items)


def get_answers(contribution, question):
    return question.answer_class.objects.filter(contribution=contribution, question=question)


def get_answers_from_answer_counters(answer_counters):
    answers = []
    for answer_counter in answer_counters:
        for __ in range(0, answer_counter.count):
            answers.append(answer_counter.answer)
    return answers


def get_textanswers(contribution, question, filter_states=None):
    assert question.is_text_question
    answers = get_answers(contribution, question)
    if filter_states is not None:
        answers = answers.filter(state__in=filter_states)
    return answers


def get_counts(answer_counters):
    if not answer_counters:
        return None

    counts = [0, 0, 0, 0, 0]
    for answer_counter in answer_counters:
        counts[answer_counter.answer - 1] = answer_counter.count
    return tuple(counts)


def get_results_cache_key(course):
    return 'evap.staff.results.tools.calculate_results-{:d}'.format(course.id)


def calculate_results(course, force_recalculation=False):
    if course.state != "published":
        return _calculate_results_impl(course)

    cache_key = get_results_cache_key(course)
    if force_recalculation:
        caches['results'].delete(cache_key)
    return caches['results'].get_or_set(cache_key, partial(_calculate_results_impl, course))


def _calculate_results_impl(course):
    """Calculates the result data for a single course. Returns a list of
    `ResultSection` tuples. Each of those tuples contains the questionnaire, the
    contributor (or None), a list of (Rating|YesNo|Text|Heading)Result tuples,
    the average grade and distribution for that section (or None)."""

    # there will be one section per relevant questionnaire--contributor pair
    sections = []

    # calculate the median values of how many people answered a questionnaire type (lecturer, tutor, ...)
    questionnaire_med_answers = defaultdict(list)
    questionnaire_max_answers = {}
    questionnaire_warning_thresholds = {}
    for questionnaire, contribution in questionnaires_and_contributions(course):
        max_answers = max([get_answers(contribution, question).aggregate(Sum('count'))['count__sum'] or 0 for question in questionnaire.rating_questions], default=0)
        questionnaire_max_answers[(questionnaire, contribution)] = max_answers
        questionnaire_med_answers[questionnaire].append(max_answers)
    for questionnaire, max_answers in questionnaire_med_answers.items():
        questionnaire_warning_thresholds[questionnaire] = max(settings.RESULTS_WARNING_PERCENTAGE * median(max_answers), settings.RESULTS_WARNING_COUNT)

    results_contain_rating_questions = False
    for questionnaire, contribution in questionnaires_and_contributions(course):
        # will contain one object per question
        results = []
        for question in questionnaire.question_set.all():
            if question.is_rating_question:
                results_contain_rating_questions = True
                answer_counters = get_answers(contribution, question)
                answers = get_answers_from_answer_counters(answer_counters)

                total_count = len(answers)
                average = avg(answers) if total_count > 0 and course.can_publish_rating_results else None
                counts = get_counts(answer_counters) if total_count > 0 and course.can_publish_rating_results else None
                warning = total_count > 0 and total_count < questionnaire_warning_thresholds[questionnaire]

                if question.is_yes_no_question:
                    if not counts:
                        approval_count = None
                    else:
                        if question.is_positive_yes_no_question:
                            approval_count = counts[0]
                        else:
                            approval_count = counts[4]
                    results.append(YesNoResult(question, total_count, average, counts, warning, approval_count))
                else:
                    results.append(RatingResult(question, total_count, average, counts, warning))

            elif question.is_text_question and course.can_publish_text_results:
                answers = get_textanswers(contribution, question, filter_states=[TextAnswer.PRIVATE, TextAnswer.PUBLISHED])
                results.append(TextResult(question=question, answers=answers))

            elif question.is_heading_question:
                results.append(HeadingResult(question=question))

        section_warning = 0 < questionnaire_max_answers[(questionnaire, contribution)] < questionnaire_warning_thresholds[questionnaire] and results_contain_rating_questions

        sections.append(ResultSection(questionnaire, contribution.contributor, contribution.label, results, section_warning))

    return sections


def normalized_distribution(distribution):
    """Returns a normalized distribution with the individual values adding up to 1.
    Can also be used to convert counts to a distribution."""
    if distribution is None:
        return None

    distribution_sum = sum(distribution)
    return tuple((value / distribution_sum) for value in distribution)


def avg_distribution(distributions, weights=itertools.repeat(1)):
    if all(distribution is None for distribution in distributions):
        return None

    summed_distribution = [0, 0, 0, 0, 0]
    for distribution, weight in zip(distributions, weights):
        if distribution:
            for index, value in enumerate(distribution):
                summed_distribution[index] += weight * value
    return normalized_distribution(summed_distribution)


def average_grade_questions_distribution(results):
    return avg_distribution([normalized_distribution(result.counts) for result in results if result.question.is_grade_question])


def average_non_grade_rating_questions_distribution(results):
    return avg_distribution([normalized_distribution(result.counts) for result in results if result.question.is_non_grade_rating_question])


def calculate_average_distribution(course):
    if not course.can_publish_average_grade:
        return None

    # will contain a list of results for each contributor and one for the course (where contributor is None)
    grouped_results = defaultdict(list)
    for __, contributor, __, results, __ in calculate_results(course):
        grouped_results[contributor].extend(results)

    course_results = grouped_results.pop(None, [])

    average_contributor_distribution = avg_distribution([
        avg_distribution(
            [average_grade_questions_distribution(results), average_non_grade_rating_questions_distribution(results)],
            [settings.CONTRIBUTOR_GRADE_QUESTIONS_WEIGHT, settings.CONTRIBUTOR_NON_GRADE_RATING_QUESTIONS_WEIGHT]
        ) for results in grouped_results.values()
    ])

    return avg_distribution(
        [average_grade_questions_distribution(course_results), average_non_grade_rating_questions_distribution(course_results), average_contributor_distribution],
        [settings.COURSE_GRADE_QUESTIONS_WEIGHT, settings.COURSE_NON_GRADE_QUESTIONS_WEIGHT, settings.CONTRIBUTIONS_WEIGHT]
    )


def distribution_to_grade(distribution):
    if distribution is None:
        return None
    return sum(answer * percentage for answer, percentage in enumerate(distribution, start=1))


def has_no_rating_answers(course, contributor, questionnaire):
    questions = questionnaire.rating_questions
    contribution = Contribution.objects.get(course=course, contributor=contributor)
    return RatingAnswerCounter.objects.filter(question__in=questions, contribution=contribution).count() == 0


def color_mix(color1, color2, fraction):
    return tuple(
        int(round(color1[i] * (1 - fraction) + color2[i] * fraction)) for i in range(3)
    )


def get_grade_color(grade):
    # Can happen if no one leaves any grades. Return white because its least likely to cause problems.
    if grade is None:
        return (255, 255, 255)
    grade = round(grade, 1)
    next_lower = int(grade)
    next_higher = int(ceil(grade))
    return color_mix(GRADE_COLORS[next_lower], GRADE_COLORS[next_higher], grade - next_lower)
